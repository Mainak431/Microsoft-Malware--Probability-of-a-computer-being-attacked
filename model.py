import pandas as pd
import numpy
import pickle
import sklearn
import math
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from keras.layers import Input, Dense, Activation,BatchNormalization
from keras.layers.core import Layer, Dense, Dropout, Activation, Flatten, Reshape,Permute
from keras.layers import Input, merge, Convolution2D, MaxPooling2D, UpSampling2D, Reshape, core, Dropout,Lambda
from keras.layers.normalization import BatchNormalization
from keras.layers.convolutional import Convolution3D, MaxPooling3D, ZeroPadding3D , ZeroPadding3D , UpSampling3D
from keras.layers.convolutional import Convolution2D, MaxPooling2D, UpSampling2D, ZeroPadding2D,Conv2DTranspose
from keras.layers.convolutional import Convolution1D, MaxPooling1D
from keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten,Conv2DTranspose,concatenate
from keras.layers.core import Dense, Dropout, Activation, Flatten
from keras.layers.convolutional import Convolution2D, MaxPooling2D,UpSampling2D
from keras.models import Sequential
from keras.optimizers import Adam,Adamax
import keras
from keras.utils import np_utils
import h5py
import csv

chunksize = 10**4
features = ['MachineIdentifier', 'ProductName', 'EngineVersion', 'AppVersion', 'AvSigVersion',
            'IsBeta', 'RtpStateBitfield', 'IsSxsPassiveMode', 'DefaultBrowsersIdentifier', 'AVProductStatesIdentifier',
            'AVProductsInstalled', 'AVProductsEnabled', 'HasTpm', 'CountryIdentifier', 'CityIdentifier', 'OrganizationIdentifier',
            'GeoNameIdentifier', 'LocaleEnglishNameIdentifier', 'Platform', 'Processor', 'OsVer', 'OsBuild', 'OsSuite', 'OsPlatformSubRelease',
            'OsBuildLab', 'SkuEdition', 'IsProtected', 'AutoSampleOptIn', 'PuaMode', 'SMode', 'IeVerIdentifier', 'SmartScreen', 'Firewall',
            'UacLuaenable', 'Census_MDC2FormFactor', 'Census_DeviceFamily', 'Census_OEMNameIdentifier', 'Census_OEMModelIdentifier', 'Census_ProcessorCoreCount',
            'Census_ProcessorManufacturerIdentifier', 'Census_ProcessorModelIdentifier', 'Census_ProcessorClass', 'Census_PrimaryDiskTotalCapacity',
            'Census_PrimaryDiskTypeName', 'Census_SystemVolumeTotalCapacity', 'Census_HasOpticalDiskDrive', 'Census_TotalPhysicalRAM', 'Census_ChassisTypeName',
            'Census_InternalPrimaryDiagonalDisplaySizeInInches', 'Census_InternalPrimaryDisplayResolutionHorizontal', 'Census_InternalPrimaryDisplayResolutionVertical',
            'Census_PowerPlatformRoleName', 'Census_InternalBatteryType', 'Census_InternalBatteryNumberOfCharges', 'Census_OSVersion', 'Census_OSArchitecture',
            'Census_OSBranch', 'Census_OSBuildNumber', 'Census_OSBuildRevision', 'Census_OSEdition', 'Census_OSSkuName',
            'Census_OSInstallTypeName', 'Census_OSInstallLanguageIdentifier', 'Census_OSUILocaleIdentifier', 'Census_OSWUAutoUpdateOptionsName',
            'Census_IsPortableOperatingSystem', 'Census_GenuineStateName', 'Census_ActivationChannel', 'Census_IsFlightingInternal',
            'Census_IsFlightsDisabled', 'Census_FlightRing', 'Census_ThresholdOptIn', 'Census_FirmwareManufacturerIdentifier',
            'Census_FirmwareVersionIdentifier', 'Census_IsSecureBootEnabled', 'Census_IsWIMBootEnabled', 'Census_IsVirtualDevice',
            'Census_IsTouchEnabled', 'Census_IsPenCapable', 'Census_IsAlwaysOnAlwaysConnectedCapable', 'Wdft_IsGamer',
            'Wdft_RegionIdentifier']
c = 0
x = []
y = []
def to_int(n) :
    n = list(n)
    sum = 0
    for i in n:
        sum += ord(i)
    return sum
"""
for chunk in pd.read_csv("train.csv", chunksize=chunksize):
    print(chunk)
    c += 1
    if c % 6 != 0:
        continue
    x.append(chunk.loc[:,features].values)
    #print(x)
    y.append(chunk.loc[:,['HasDetections']].values)
    #print(chunk.loc[:,['HasDetections']].values)
y = numpy.array(y)
x = numpy.array(x)
#x = x.reshape(40,82)
#print(x)
#pickle_out = open("x1","wb")
#pickle.dump(x,pickle_out)
#pickle_out = open("y1","wb")
#pickle.dump(y,pickle_out)
print(x.shape)
print(y.shape)
#with open('x1','rb') as f:
#    x = pickle.load(f)
#with open('y1','rb') as f:
#    y = pickle.load(f)
print(x.shape)
print(y.shape)
x = x.reshape(x.shape[0] * x.shape[1],x.shape[2])
y = y.reshape(y.shape[0] * y.shape[1],y.shape[2])
print(x.shape)
print(y.shape)
#pickle_out = open("x1","wb")
#pickle.dump(x,pickle_out)
#pickle_out = open("y1","wb")
#pickle.dump(y,pickle_out)

with open('x1','rb') as f:
    x = pickle.load(f)
with open('y1','rb') as f:
    y = pickle.load(f)
m, n = x.shape
m1,n1 = y.shape
print(x.shape)

z = x.max(axis = 0)
c = 0
for i in range(x.shape[0]):
    c += 1
    print(c)
    for j in range(x.shape[1]):
        if math.isnan(x[i][j]) :
            print("Nan")
            x[i][j] = 0
        elif isinstance(x[i][j],int) or isinstance(x[i][j],float) :
            continue
        else :
            x[i][j] = to_int(x[i][j])


#pickle_out = open("x1","wb")
#pickle.dump(x,pickle_out)
#pickle_out = open("y1","wb")
#pickle.dump(y,pickle_out)


#with open('x1','rb') as f:
#    x = pickle.load(f)
#with open('y1','rb') as f:
#    y = pickle.load(f)
print(x)
print(y)

x = numpy.array(x,dtype=numpy.float64)
x_mean = x.mean(axis=0)
x_std =  x.std(axis = 0)


x_max = x.max(axis = 0)
x -= x_mean
x /= x_std

pickle_out = open("x1","wb")
pickle.dump(x,pickle_out)

pickle_out = open("x_mean","wb")
pickle.dump(x_mean,pickle_out)

pickle_out = open("x_std","wb")
pickle.dump(x_std,pickle_out)

print(x)
"""
x = []
y = []
with open('x_train1', 'rb') as f:
    x = pickle.load(f)
m,n = x.shape
"""inputs = keras.layers.Input(shape=(n,1))
pool4 = Dense(1024,activation = 'elu')(inputs)
#pool4 = Dense(1024)(pool4)
#pool4 = Dropout(0.5)(pool4)
pool4 = Dense(512,activation = 'elu')(pool4)
#pool4 = Dropout(0.5)(pool4)
pool4 = Dense(256,activation = 'elu')(pool4)
pool4 = Dense(128,activation = 'elu')(pool4)
#pool4 = Dropout(0.5)(pool4)
pool4 = Dense(64,activation = 'elu')(pool4)
#pool4 = Dropout(0.5)(pool4)
pool4 = Dense(32,activation = 'elu')(pool4)
#pool4 = Dropout(0.5)(pool4)
pool4 = Dense(16,activation = 'elu')(pool4)
pool4 = Dense(8,activation = 'elu')(pool4)
#pool4 = Dropout(0.5)(pool4)
pool4 = Dense(4,activation = 'elu')(pool4)
pool4 = Dense(2)(pool4)
pool4 = Dropout(0.5)(pool4)
pool4 = Flatten()(pool4)
pool4 = Dense(2,activation="softmax")(pool4)
print(pool4.shape)
model = keras.Model(input=inputs,output=pool4)
model.compile(optimizer='adamax', loss='binary_crossentropy', metrics=['accuracy'])"""
# number of convolutional filters to use
nb_filters = 32
# size of pooling area for max pooling
nb_pool = 3
# convolution kernel size
nb_conv = 3
input_case = keras.layers.Input(shape=(n,1))

tower1 = Dense(2 * nb_filters, activation='elu')(input_case)
tower1 = Dense(2 * nb_filters, activation='elu')(tower1)
p1 = tower1
#tower1 = MaxPooling2D(pool_size=(nb_pool, nb_pool))(tower1)
tower1 = Dropout(0.5)(tower1)

tower1 = Dense(2 * nb_filters, activation='elu')(tower1)
tower1 = Dense(2 * nb_filters, activation='elu')(tower1)
p4 = tower1
#tower1 = MaxPooling2D(pool_size=(nb_pool, nb_pool))(tower1)
tower1 = Dropout(0.5)(tower1)

tower1 = Dense(2 * nb_filters, activation='elu')(tower1)
tower1 = Dense(2 * nb_filters, activation='elu')(tower1)
#tower1 = MaxPooling2D(pool_size=(nb_pool, nb_pool))(tower1)
tower1 = Dropout(0.5)(tower1)

#t1.append(tower1)
# merged = keras.layers.concatenate([tower1, tower1, tower2], axis=1)


tower2 = Dense(2 * nb_filters, activation='elu')(input_case)
tower2 = Dense(2 * nb_filters, activation='elu')(tower2)
p2 = tower2
#tower2 = MaxPooling2D(pool_size=(nb_pool, nb_pool))(tower2)
tower2 = Dropout(0.5)(tower2)

tower2 = Dense(2 * nb_filters, activation='elu')(tower2)
tower2 = Dense(2 * nb_filters, activation='elu')(tower2)
#tower2 = MaxPooling2D(pool_size=(nb_pool, nb_pool))(tower2)
# tower2 = Dropout(0.5)(tower2)
# print(tower2.shape)
#t2.append(tower2)
# tower2 = Conv2D(2*nb_filters, (nb_conv, nb_conv), padding='same', activation='elu', input_shape=(img_rows, img_cols,img_channels))(tower2)
# tower2 = Conv2D(2*nb_filters, (nb_conv, nb_conv), padding='same', activation='elu', input_shape=(img_rows, img_cols,img_channels))(tower2)
#tower2 = MaxPooling2D(pool_size=(nb_pool, nb_pool))(tower2)
tower2 = Dropout(0.5)(tower2)

# tower3 = input_case


tower3 = Dense(2 * nb_filters, activation='elu')(input_case)
tower3 = Dense(2 * nb_filters, activation='elu')(tower3)
p3 = tower3
# tower3 = Conv2D(2*nb_filters, (nb_conv, nb_conv), padding='same', activation='elu', input_shape=(img_rows, img_cols,img_channels))(tower3)
#tower3 = MaxPooling2D(pool_size=(nb_pool, nb_pool))(tower3)
tower3 = Dropout(0.5)(tower3)
# print(tower3.shape)
#t3.append(tower3)
# tower3 = Conv2D(2*nb_filters, (nb_conv, nb_conv), padding='same', activation='elu', input_shape=(img_rows, img_cols,img_channels))(tower3)
# tower3 = Conv2D(2*nb_filters, (nb_conv, nb_conv), padding='same', activation='elu', input_shape=(img_rows, img_cols,img_channels))(tower3)
#tower3 = MaxPooling2D(pool_size=(nb_pool, nb_pool))(tower3)
# tower3 = Dropout(0.5)(tower3)

# tower3 = Conv2D(2*nb_filters, (nb_conv, nb_conv), padding='same', activation='elu', input_shape=(img_rows, img_cols,img_channels))(tower3)
# tower3 = Conv2D(2*nb_filters, (nb_conv, nb_conv), padding='same', activation='elu', input_shape=(img_rows, img_cols,img_channels))(tower3)
#tower3 = MaxPooling2D(pool_size=(nb_pool, nb_pool))(tower3)
tower3 = Dropout(0.5)(tower3)
merged = keras.layers.concatenate([tower1, tower2, tower3,p1,p2,p3,p4], axis=1)
p = merged

tower4 = Dense(2 * nb_filters, activation='elu')(p)
tower4 = Dense(2 * nb_filters, activation='elu')(tower4)

#tower4 = MaxPooling2D(pool_size=(nb_pool, nb_pool))(tower4)
tower4 = Dropout(0.5)(tower4)

tower4 = Dense(2 * nb_filters, activation='elu')(tower4)
tower4 = Dense(2 * nb_filters, activation='elu')(tower4)
#tower4 = MaxPooling2D(pool_size=(nb_pool, nb_pool))(tower4)
tower4 = Dropout(0.5)(tower4)

tower4 = Dense(2 * nb_filters, activation='elu')(tower4)
tower4 = Dense(2 * nb_filters, activation='elu')(tower4)
#tower4 = MaxPooling2D(pool_size=(nb_pool, nb_pool))(tower4)
tower4 = Dropout(0.5)(tower4)



print(tower3.shape)


tower5 = Dense(2 * nb_filters, activation='elu')(p)
tower5 = Dense(2 * nb_filters, activation='elu')(tower5)

#tower5 = MaxPooling2D(pool_size=(nb_pool, nb_pool))(tower5)
tower5 = Dropout(0.5)(tower5)

tower5 = Dense(2 * nb_filters, activation='elu')(tower5)
tower5 = Dense(2 * nb_filters, activation='elu')(tower5)

#tower5 = MaxPooling2D(pool_size=(nb_pool, nb_pool))(tower5)
# tower5 = Dropout(0.5)(tower5)
# print(tower5.shape)
#t2.append(tower5)
# tower5 = Conv2D(2*nb_filters, (nb_conv, nb_conv), padding='same', activation='elu', input_shape=(img_rows, img_cols,img_channels))(tower5)
# tower5 = Conv2D(2*nb_filters, (nb_conv, nb_conv), padding='same', activation='elu', input_shape=(img_rows, img_cols,img_channels))(tower5)
#tower5 = MaxPooling2D(pool_size=(nb_pool, nb_pool))(tower5)
tower5 = Dropout(0.5)(tower5)

tower6 = Dense(2 * nb_filters, activation='elu')(p)
tower6 = Dense(2 * nb_filters, activation='elu')(tower6)
tower6 = Dropout(0.5)(tower6)
tower6 = Dropout(0.5)(tower6)
merged = keras.layers.concatenate([tower4, tower5, tower6, p,tower1,tower2,tower3], axis=1)

merged = Flatten()(merged)

out = Dense(512, activation='elu')(merged)
out = Dropout(0.5)(out)
out = Dense(2, activation='softmax')(out)
model = keras.Model(input_case, out)
model.compile(optimizer='adamax', loss='binary_crossentropy', metrics=['accuracy'])
model.summary
nb_epoch = 30
batch_size = 800
nb_classes = 2
for ab in range(6) :
    with open('x_train'+ str(ab + 1), 'rb') as f:
        x = pickle.load(f)
    with open('y_train' + str(ab + 1), 'rb') as f:
        y = pickle.load(f)
    y = y.reshape(y.shape[0] * y.shape[1] * 1)
    x = numpy.float32(x)
    m, n = x.shape
    print(x.shape)
    #logistic.fit(x,y)

    #x = x.reshape(1,x.shape[0],x.shape[1],1)
    #y = y.reshape(1,y.shape[0],y.shape[1],1)

    #y = numpy.array(y,dtype=numpy.float64)
    y = np_utils.to_categorical(y , nb_classes)
    #y_test = np_utils.to_categorical(y_test , nb_classes)
    x = x.reshape(x.shape[0], x.shape[1],1)
    #x_test = x_test.reshape(x_test.shape[0], x_test.shape[1],1)

    #y = y.reshape(y.shape[0],y.shape[1])
    #y_test = y_test.reshape(y_test.shape[0],y_test.shape[1])
    print(y)

    #logistic  = LogisticRegression(solver= 'lbfgs')
    #logistic.fit(x_train,y_train)

    model.fit(x, y, batch_size=batch_size, epochs=nb_epoch, verbose=1)


fname = "weights.hdf5"
model.save_weights(fname, overwrite=True)
model.load_weights(fname)

#print(float(count)/float(x_test.shape[0]))
with open('result/ann/result.csv', 'wb') as csvfile:
    spamwriter = csv.writer(csvfile, delimiter=' ')
    spamwriter.writerow(['MachineIdentifier'] +[',']+ ['HasDetections'])
    for ab in range(6):
        r = []
        with open('label'+ str(ab + 1),'rb') as f:
            r = pickle.load(f)
        r = r[:,0]
        print(r)
        test = []
        with open('x_test' + str(ab + 1),'rb') as f:
            test = pickle.load(f)
        #test = pca.transform(test)
        test = test.reshape(test.shape[0], test.shape[1], 1)
        print (test.shape)
        predictions = model.predict(test)[:,1]
        print(predictions)
        print(model.predict(test))
        w = numpy.full(r.shape[0], ',', dtype=object)
        z = numpy.empty([r.shape[0], 3], dtype=object)
        z[:,0] = r
        z[:,2] = predictions
        z[:,1] = w
        spamwriter.writerows(z)

reader = []
with open('result/ann/result.csv', 'r') as f:
    reader = csv.reader(f, delimiter=',', quoting=csv.QUOTE_NONE)
    reader = [[x.strip() for x in row] for row in reader]

#print(reader)

with open('result/ann/result.csv', 'wb') as csvfile:
    spamwriter = csv.writer(csvfile, delimiter=',')
    spamwriter.writerows(reader)